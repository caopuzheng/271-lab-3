---
title: 'Lab 3: Panel Models'
subtitle: 'US Traffic Fatalities: 1980 - 2004'
output: 
  bookdown::pdf_document2: default
---

```{r load packages, echo=FALSE, message=FALSE, warning=FALSE}
library(plm)
library(lmtest)
library(stargazer)
library(tidyverse)
library(patchwork)
library(magrittr)
library(dplyr)

if (!"ggthemes" %in% rownames(installed.packages())) {
  install.packages("ggthemes")
}
library(ggthemes)

if (!"scales" %in% rownames(installed.packages())) {
  install.packages("scales")
}
library(scales)

if (!"mgcv" %in% rownames(installed.packages())) {
  install.packages("mgcv")
}
library(mgcv)

if (!"reshape2" %in% rownames(installed.packages())) {
  install.packages("reshape2")
}
library(reshape2)


if (!"ggplot2" %in% rownames(installed.packages())) {
  install.packages("ggplot2")
}
library(ggplot2)

if (!"gridExtra" %in% rownames(installed.packages())) {
  install.packages("gridExtra")
}
library(gridExtra)

if (!"tibble" %in% rownames(installed.packages())) {
  install.packages("tibble")
}
library(tibble)

if (!"lme4" %in% rownames(installed.packages())) {
  install.packages("lme4")
}
library(lme4)

if (!"stargazer" %in% rownames(installed.packages())) {
  install.packages("stargazer")
}
library(stargazer)

library(lubridate)

require(knitr)
knitr::opts_chunk$set(tidy = TRUE, tidy.opts = list(comment = FALSE))
knitr::opts_chunk$set(comment = " ")
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```


# U.S. traffic fatalities: 1980-2004

In this lab, we are asking you to answer the following **causal** question: 

> **"Do changes in traffic laws affect traffic fatalities?"**  

```{asis, echo=FALSE, include = FALSE}
To answer this question, please complete the tasks specified below using the data provided in `data/driving.Rdata`. This data includes 25 years of data that cover changes in various state drunk driving, seat belt, and speed limit laws. 

Specifically, this data set contains data for the 48 continental U.S. states from 1980 through 2004. Various driving laws are indicated in the data set, such as the alcohol level at which drivers are considered legally intoxicated. There are also indicators for “per se” laws—where licenses can be revoked without a trial—and seat belt laws. A few economics and demographic variables are also included. The description of the each of the variables in the dataset is also provided in the dataset. 
```
> In order to answer this research question, we analyzed the 1980-2004 U.S. traffic fatalities panel dataset for 48 states. We conducted comprehensive exploratory data analysis and visually observed the relationship of the potential predictors with traffic fatalities. We also developed several models to estimate the effect of the potential predictors. Based on our study, we believe that changes in traffic laws affect traffic fatalities rate. After evaluating linear models, fixed effects model and random effects model, we believe that the fixed effects model is the most appropriate model for this dataset. See details in the sections below.

```{r load data, echo = TRUE, include = FALSE}
load(file = "./data/driving.RData")
## please comment these calls in your work
# glimpse(data)
# desc
```


# (30 points, total) Build and Describe the Data 

1. (5 points) Load the data and produce useful features. 
```{asis, echo=FALSE}
    - Produce a new variable, called `speed_limit` that re-encodes the data that is in `sl55`, `sl65`, `sl70`, `sl75`, and `slnone`; 
    - Produce a new variable, called `year_of_observation` that re-encodes the data that is in `d80`, `d81`, ... , `d04`. 
    - Produce a new variable for each of the other variables that are one-hot encoded (i.e. `bac*` variable series). 
    - Rename these variables to sensible names that are legible to a reader of your analysis. For example, the dependent variable as provided is called, `totfatrte`. Pick something more sensible, like, `total_fatalities_rate`. There are few enough of these variables to change, that you should change them for all the variables in the data. (You will thank yourself later.)
```
> We performed comprehensive exploratory data analysis of this dataset. Please see Appendix 1.1 for the code details. We summarized the key notes about our data processing work as follows:

```{r data-processing, include=FALSE}
# Appendix 1.1
# produce new variables - year_of_observation, speed_limit, speed_limit_70plus, blood_alcohol_limit
df <- data %>%
  mutate(state = factor(state)) %>%
  rowwise() %>%
  # speed_limit
  mutate(
    speed_limit_70plus = factor(sl70plus),
    speed_limit = parse_number(
      colnames(
        select(data, starts_with("sl"))
      )[which.max(c_across(starts_with("sl")))],
      na = "slnone"
    ),
  ) %>%
  select(-starts_with("sl")) %>%
  mutate(year_of_observation = factor(year)) %>% # year_of_observation
  select(-starts_with("d")) %>%
  mutate(blood_alcohol_limit = factor(parse_number(
    colnames(
      select(data, starts_with("bac"))
    )[which.max(c_across(starts_with("bac")))]
  ) / 100)) %>% # blood_alcohol_limit
  select(-starts_with("bac")) %>%
  mutate(
    seatbelt = factor(seatbelt), # 'seatbelt' categorizes primary or secondary
    speed_limit_70plus = ifelse(speed_limit == 55 | speed_limit == 65, 0, 1)
  ) %>%
  select(-starts_with("sb"))

# rename the variables to sensible names
df <- df %>%
  dplyr::rename(
    "total_fatalities_rate" = "totfatrte",
    "minimum_drinking_age" = "minage",
    "zero_tolerance_law" = "zerotol",
    "graduated_drivers_license_law" = "gdl",
    "per_se_laws" = "perse",
    "total_traffic_fatalities" = "totfat",
    "total_nighttime_fatalities" = "nghtfat",
    "total_weekend_fatalities" = "wkndfat",
    "total_fatalities_per_100_million_miles" = "totfatpvm",
    "nighttime_fatalities_per_100_million_miles" = "nghtfatpvm",
    "weekend_fatalities_per_100_million_miles" = "wkndfatpvm",
    "nighttime_fatalities_rate" = "nghtfatrte",
    "weekend_fatalities_rate" = "wkndfatrte",
    "vehicle_miles" = "vehicmiles",
    "unemployment_rate" = "unem",
    "pct_population_14_to_24" = "perc14_24",
    "vehicle_miles_per_capita" = "vehicmilespc"
  ) %>%
  select(
    year_of_observation,
    state,
    year,
    # response variables
    total_fatalities_rate,
    nighttime_fatalities_rate,
    weekend_fatalities_rate,
    total_traffic_fatalities,
    total_nighttime_fatalities,
    total_weekend_fatalities,
    total_fatalities_per_100_million_miles,
    nighttime_fatalities_per_100_million_miles,
    weekend_fatalities_per_100_million_miles,
    # potential explanatory variables
    seatbelt,
    zero_tolerance_law,
    graduated_drivers_license_law,
    per_se_laws,
    minimum_drinking_age,
    speed_limit_70plus,
    speed_limit,
    blood_alcohol_limit,
    vehicle_miles,
    vehicle_miles_per_capita,
    # econ and demographic variables
    statepop,
    unemployment_rate,
    pct_population_14_to_24, vehicle_miles
  ) # keep the similar variables together

# check the data
# df %>% glimpse()
```

```{r include=FALSE}
check_sl65 <- data %>%
  filter(sl65 > 0 & sl65 < 1) %>%
  select(sl55, sl65, sl70, sl75, slnone) %>%
  mutate(sum_sl = sum(sl55, sl65, sl70, sl75, slnone))
# head(check_sl65, 2)
```


> 1) We created new variables and renamed variables using sensibel variable names.

> 2) Some rows of the traffice speed columns (sl55/sl65/sl70/s75/slone) have percentage values. These speed columns are categorical variables and should reflect binary values (0, 1). We classified the percentage values based on the field with the max value.

> 3) The speed_limit_70plus column has values that are not 0 or 1. We reclassified these values to 0 or 1, based on speed_limit column. We also reclassified the states with no speed limit as 1 in this column.

```{r data-diagnose-and-cleanup categorical fields, include=FALSE}
# check the values of these fields
# df$zero_tolerance_law %>%
#   table(useNA = "ifany") %>%
#   as.data.frame()
#
# df$graduated_drivers_license_law %>%
#   table(useNA = "ifany") %>%
#   as.data.frame()
#
# df$per_se_laws %>%
#   table(useNA = "ifany") %>%
#   as.data.frame()

# make binary variables
df <- df %>%
  mutate(
    zero_tolerance_law = ifelse(
      zero_tolerance_law == 0 | zero_tolerance_law == 1, zero_tolerance_law, 1
    ),
    graduated_drivers_license_law = ifelse(
      graduated_drivers_license_law == 0 | graduated_drivers_license_law == 1,
      graduated_drivers_license_law,
      1
    ),
    per_se_laws = ifelse(
      per_se_laws == 0 | per_se_laws == 1, per_se_laws, 1
    ),
    blood_alcohol_limit_binary = ifelse(
      blood_alcohol_limit == 0.1, 1, 0
    ) # set 1 for 0.1 blood_alcohol_limit and 0 for 0.08 blood_alcohol_limit
  )
```

> 4) We observed non-binary (percentage) values in the three law columns (zero_tolerance_law, graduated_drivers_license_law, per_se_laws). Since we expect these columns to have binary values (0,1), we reclassified all non-zero values as 1 to reflect these fields as binary variables. The minimum_drinking_age column has values that are not integers and we rounded these values to the nearest integer. 

```{r minimum-drinking-age, include=FALSE}
# check minimum_drinking_age
# df$minimum_drinking_age %>%
#   table(useNA = "ifany") %>%
#   as.data.frame()

df <- df %>%
  mutate(
    minimum_drinking_age = round(minimum_drinking_age, 0)
  )
```


```{r missing-speed-limit, echo=FALSE, include=FALSE}
# check for na speed_limit rows
check_na_speed <- df %>%
  filter(is.na(speed_limit)) %>%
  select(state, year_of_observation, speed_limit, speed_limit_70plus)

# head(check_na_speed,2)

# treat the na speed_limit rows for State 27
df <- df %>%
  mutate(
    speed_limit = ifelse(
      is.na(speed_limit) & state == 27, ifelse(
        year >= 1996 & year <= 1999, 85, 75
      ), speed_limit
    ),
    speed_limit_70plus = ifelse(
      is.na(speed_limit_70plus) & state == 27, 1, speed_limit_70plus
    )
  )
```

> 5) The $speed\_limit$ is not set for State 27 between 1996 to 2004. Based on our research, we noted that: for three years after the 1995 repeal of the increased 65 mph limit, Montana had a non-numeric "reasonable and prudent" speed limit during the daytime on most rural roads. But it doesn't mean there was no speed limit. We decided to set the $speed\_limit$ to 85 for Montana between 1996 to 1999, given the legal case of [State v. Rudy Stanko (1998)](https://en.wikipedia.org/wiki/Speed_limits_in_the_United_States_by_jurisdiction), who got charged for speed of 85 mph. Effective May 28, 1999, as a result of that decision, the Montana Legislature established a speed limit of 75 mph. So we set the $speed\_limit$ to 75 for Montana between 2000 to 2004.

2. (5 points) Provide a description of the basic structure of the dataset.
```{asis, echo=FALSE}
 What is this data? How, where, and when is it collected? Is the data generated through a survey or some other method? Is the data that is presented a sample from the population, or is it a *census* that represents the entire population? Minimally, this should include:
    - How is the our dependent variable of interest `total_fatalities_rate` defined? 
```

> This data set is a balanced longitudinal dataset and contains traffic fatalities data for the 48 continental U.S. states from 1980 through 2004. For each year of observation, the dataset contains state-level cross sectional measurements of fatality count and rate. This data is collected and distributed by Jeffrey M. Wooldridge through this [link](https://github.com/JustinMShea/wooldridge). In this dataset, the total_fatalities_rate is defined as total fatalities per 100,000 population.	

>  Wooldridge sourced the data from the National Highway Traffic Safety Administration (NHTSA). Since the early 1980s, NHTSA has been obtaining, from various states, computer data files coded from police accident reports.  NHTSA maitains these data files at its National Center for Statistics and Analysis (NCSA). NCSA’s goal is to include all states in the program over time, providing a complete census of national traffic statistics. See this [link](https://onlinelibrary.wiley.com/doi/epdf/10.1111/j.1465-7287.2007.00039.x?saml_referrer=)

> After our data processing work, the clean dataset has 25 columns, see Appendix 1.2. These fields include: 1) index variables (year_of_observation, state, year); 2) nine fatality variables: There are three measurements (fatality count, fatality count per 100M miles and fatality rate). Each measurement is provided for total, nighttime and weekend; 3) eight traffic laws indicators (seatbelt, zero_tolerance_law, graduated_drivers_license_law, per_se_laws, minimum_drinking_age, speed_limit_70plus, speed_limit, blood_alcohol_limit), two driving variables (vehicle_miles, vehicle_miles_per_capita), and three economics and demographic variables (statepop, unemployment_rate, pct_population_14_to_24). 

```{r clean-data, include=FALSE}
# Appendix 1.2
glimpse(df)
```


3. (20 points) Conduct a very thorough EDA.
```{asis, echo=FALSE}
 which should include both graphical and tabular techniques, on the dataset, including both the dependent variable `total_fatalities_rate` and the potential explanatory variables. Minimally, this should include: 
  - How is the our dependent variable of interest `total_fatalities_rate` defined? 
```
> In this dataset, the total_fatalities_rate is defined as total fatalities per 100,000 population.	   

```{asis, echo=FALSE}
- What is the average of `total_fatalities_rate` in each of the years in the time period covered in this dataset? 
```
> Figure 1 below shows the average of total_fatalities_rate by year, with a declining trend visually.

```{r avg-fatality-by-year, warning=FALSE, fig.align='center', out.width="55%", echo=FALSE, fig.cap="Average Total Fatalities Rates 1980-2004"}
df_avg <- df %>%
  group_by(year_of_observation) %>%
  summarise(avg_total_fatalities_rate = mean(total_fatalities_rate))

# average fatality by year
years <- unique(df$year_of_observation)
avg_df <- data.frame(
  year = years,
  avg_fatality_rate = round(df_avg$avg_total_fatalities_rate, 2)
) %>%
  knitr::kable()

# plot fatality by year
df_avg %>%
  ggplot(aes(year_of_observation, avg_total_fatalities_rate,
    label = round(avg_total_fatalities_rate, 2)
  )) +
  geom_point() +
  geom_text(
    hjust = 0, vjust = -0.5, size = 2.5
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
  labs(title = "Average Total Fatalities Rate by Year") +
  xlab("Year") +
  ylab("Average Total Fatalities Rate %")
```

> Next we examined the histograms of the continuous variables and noted that most of these variables are right-skewed. Data transformation (e.g. log) can be helpful.  
  
```{r distribution-plots1, warning=FALSE, message=FALSE, fig.align='center', out.width="55%", echo=FALSE, fig.cap="Continuous Explanatory Variables Distribution"}
df %>%
  select(
    total_fatalities_rate,
    nighttime_fatalities_rate,
    weekend_fatalities_rate,
    vehicle_miles,
    vehicle_miles_per_capita,
    statepop,
    unemployment_rate,
    pct_population_14_to_24
  ) %>%
  keep(is.numeric) %>%
  gather() %>%
  ggplot(aes(value)) +
  facet_wrap(~key, scales = "free") +
  geom_histogram()

# hist(df$pct_population_14_to_24)
# hist(log(df$pct_population_14_to_24))
```

> We also examined the distribution of the categorical variables (see Appendix 1.3). We observed: 1) Blood alcohol limit: Most states have the limit of 0.1; 2)Minimum drinking age: Most states have 21; 3) Graduated drivers license law: Most states have 0;4) Speed limit: Most states have less than 70 miles; 5) per-se_laws, seatbelt and zero_tolence_law are more evenly distributed among their categories.

```{r, categorical-variables, warning=FALSE,include=FALSE}
# Appendix 1.3
df %>%
  select(
    seatbelt,
    zero_tolerance_law,
    graduated_drivers_license_law,
    per_se_laws,
    minimum_drinking_age,
    speed_limit_70plus,
    speed_limit,
    blood_alcohol_limit
  ) %>%
  gather() %>%
  ggplot(aes(value)) +
  facet_wrap(~key, scales = "free") +
  geom_bar()
```


> We plotted total_fatalities_rate by state below to understand the fixed effects by state (see Appendix 1.4). We observed strong differences in total traffic fatalities rates across states, suggesting that state-level fixed effects are important for controlling for unobserved differences.

```{r, total_fatalities_rate-by-state, warning=FALSE, include=FALSE}
# Appendix 1.4
df %>%
  ggplot(aes(reorder(state, desc(total_fatalities_rate)), total_fatalities_rate,
    fill = state
  )) +
  geom_boxplot(alpha = 0.4) +
  theme_economist_white(gray_bg = FALSE) +
  theme(legend.position = "none", axis.text.y = element_text(size = 6)) +
  scale_y_continuous(label = percent) +
  xlab("State") +
  ylab("Total Fatalities Rate%") +
  coord_flip()
```


> We also plotted total_fatalities_rate by state in Figure 3 to understand the trend over the years. All plots are sequentially ordered, despite of missing state number 2, 9, and 12. Most states have a downward trend in total fatalities rate over the years, with trend variations by state.

```{r cross-sectional-plot, fig.align='center', out.width="70%", echo=FALSE, fig.cap="Total Fatality Rate by State"}
# we do have 48 states -> missing states 2, 9, and 12.
# Thus, the ordering is a little bit off
df %>%
  ggplot(aes(year, total_fatalities_rate, color = state)) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm") +
  facet_wrap(~state) +
  theme_economist_white(gray_bg = FALSE) +
  theme(
    legend.position = "none",
    axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, size = 8),
    axis.text.y = element_text(size = 8),
    strip.text = element_text(size = 8)
  ) +
  scale_y_continuous() +
  xlab("State") +
  ylab("Total Fatalities Rate %")
```


> Figure 4 below compares the original data vs. log-transformed data of several variables: vehicle_miles, vehicle_miles_per_capita, statepop, pct_population_14_to_24, unemployment_rate. The log transformation seemed to improve the relationship between some of these explanatory variables and total fatality rates, as the original data was more skewed. We decided to use log transformation for our interpretation of vehicle_miles, vechicle_miles_per_capita, statepop and unemployment_rate.

> Total fatalities rate is positively correlated and percentage of population aged 14-24. Total fatalities rate is negatively correlated with vehicle miles and state population which is not intuitive. We interpret this as population and vehicle miles are both increasing with time, so are other potential factors (e.g. car quality, technology, road conditions, etc.). we noted total fatalities rate is positively correlated with vehicle miles per capita, which is consistent with our background knowledge. Therefore, we will use vehicle miles per capita, instead of vehicle miles and state population separately.

```{r pairplot-on-feature-of-interest-continuous, fig.align='center', out.width="65%", echo=FALSE, fig.cap="Continous Explanatory Variables Effect on Total Fatality Rate"}
df %>%
  mutate(
    vehicle_miles_log = log(vehicle_miles),
    vehicle_miles_per_capita_log = log(vehicle_miles_per_capita),
    statepop_log = log(statepop),
    pct_population_14_to_24_log = log(pct_population_14_to_24),
    unemployment_rate_log = log(unemployment_rate)
  ) %>%
  select(
    total_fatalities_rate,
    nighttime_fatalities_rate,
    weekend_fatalities_rate,
    vehicle_miles,
    vehicle_miles_log,
    vehicle_miles_per_capita,
    vehicle_miles_per_capita_log,
    statepop,
    statepop_log,
    pct_population_14_to_24,
    pct_population_14_to_24_log,
    unemployment_rate,
    unemployment_rate_log
  ) %>%
  melt(id.vars = c("total_fatalities_rate")) %>%
  ggplot(aes(value, total_fatalities_rate, color = variable)) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm") +
  facet_wrap(~variable, scales = "free_x") +
  # theme_economist_white(gray_bg=F) +
  theme(
    legend.position = "none",
    axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, size = 8),
    axis.text.y = element_text(size = 8),
    strip.text = element_text(size = 8)
  ) +
  # scale_y_continuous(label=percent) +
  xlab("Continous Feature vs Total Fatalities Rate") +
  ylab("Total Fatality Rate")
```

```{r save-log-transformation, include=FALSE}
df <- df %>%
  mutate(
    vehicle_miles_log = log(vehicle_miles),
    vehicle_miles_per_capita_log = log(vehicle_miles_per_capita),
    statepop_log = log(statepop),
    unemployment_rate_log = log(unemployment_rate)
  )
```

> The boxplots (Figure 5) displays the differences in total fatalities rate for the groups within each law categorical variable. We observed that stricter law requirements tend to associate with lower total fatalities rate, see below the detailed comments for each law variable.

> 1) Seatbelt: No seatbelt requirements tend to have higher fatalities rate than primary and secondary seatbelt requirements;  2) For zero_tolerance_law, graduated_drivers_license_law and per_se_laws: 0 (stricter laws) tends to have lower fatalities rate, comparedto having some level of tolerance in law requirements;  3) minimum_drinking_age: Higher legally-eligible age tends to have lower fatalities rate than lower age limit; 4) speed_limit_70plus: Below 70 speed limit tends to have lower fatalities rate than those with 70+ speed limit; 5) blood_alcohol_limit: 0.08 blood alcohol limit tends to have lower fatalities rate, compared to 0.1 blood alcohol limit.

```{r pairplot-on-feature-of-interest-discrete, fig.align='center', out.width="70%", warning=FALSE, echo=FALSE, fig.cap="Categorical Variables' Effect on Total Fatalities Rate"}
df %>%
  select(
    total_fatalities_rate,
    seatbelt,
    zero_tolerance_law,
    graduated_drivers_license_law,
    per_se_laws,
    minimum_drinking_age,
    speed_limit_70plus,
    speed_limit,
    blood_alcohol_limit
  ) %>%
  melt(id.vars = c("total_fatalities_rate")) %>%
  ggplot(aes(value, total_fatalities_rate)) +
  geom_boxplot(aes(fill = factor(value))) +
  coord_flip() +
  facet_wrap(~variable, scales = "free") +
  theme(
    legend.position = "none",
    axis.text.x = element_text(size = 8),
    axis.text.y = element_text(size = 8),
    strip.text = element_text(size = 8)
  ) +
  xlab("Groups for Each Categorical Variables ") +
  ylab("Total Fatality Rate")
```

> We examined the correlation matrix (Figure 6) of the continuously variables to understand correlation between variables.  State population and vehicle miles are almost perfectly correlated. To avoid perfect colinearity (and the reasons stated above in Figure 4), we will only use vehicle_miles_per_capita in our model development.

```{r compute-pearson-correlation-confusion-matrix, fig.align='center', out.width="85%", echo=FALSE, fig.cap="Pearson Correlation"}
df %>%
  mutate(seatbelt_num = as.numeric(seatbelt)) %>%
  select(
    total_fatalities_rate,
    nighttime_fatalities_rate,
    weekend_fatalities_rate,
    vehicle_miles_log,
    statepop_log,
    vehicle_miles_per_capita_log,
    unemployment_rate_log,
    pct_population_14_to_24,
  ) %>%
  mutate_all(as.numeric) %>%
  cor() %>%
  melt() %>%
  ggplot(aes(Var1, Var2, fill = value)) +
  geom_tile() +
  theme_economist_white(gray_bg = FALSE) +
  theme(
    legend.title = element_blank(),
    legend.text = element_text(size = 10),
    axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)
  ) +
  scale_fill_gradient2(
    low = "cornflowerblue", high = "coral", mid = "white",
    midpoint = 0, limit = c(-1, 1)
  )
```



```{asis, echo=FALSE}
As with every EDA this semester, the goal of this EDA is not to document your own process of discovery -- save that for an exploration notebook -- but instead it is to bring a reader that is new to the data to a full understanding of the important features of your data as quickly as possible. In order to do this, your EDA should include a detailed, orderly narrative description of what you want your reader to know. Do not include any output -- tables, plots, or statistics -- that you do not intend to write about.
```

> To summarize our data exploratory work, we noted the following key takeaways:

> 1) We pre-processed the variables in the original dataset, including creating required new variables, cleaning up categorical variables for consistency and renaming all variables with sensible names. Our final dataset in the df file have 25 variables, including index variables (year/year-of-observations and state), the fatality rates variables and the potential explanatory variables (law, economics and demographic variables). We noted 48 states are included in this dataset, which is consistent with the data lable (48 continental states).

> 2) We conducted comprehensive data exploratory work by using extensive plots to visualize and examine the distribution of each variable, relationship between the total fatalities rate and the potential explanatory variables (including the log form of several variables), as well as the correlation among the variables. 

> 3) We noted a declining trend of the average of total fatalities rates over the years, with state-level trend variations.  However, year/time by itself does not fully explain fatalities rates. There are many other factors that affect the total fatalities rates over the years.  We had three key observations that could explain the changes in total fatalities rates over the period: a) stricter traffic laws (as indicated in seatbelt, zero_tolerance_law, graduated_drivers_license_law, per_se_laws, minimum_drinking_age, speed_limit_70plus, blood_alcohol_limit) tend to associate with lower total fatalities rate. b) there is a sensible relationship between total fatalities rate and each of the three economics/demographic variables (pct_population_14_to_24, unemployment_rate_log and vehicle_miles_per_capita_log). and finally c) the improvement of technology and road conditions over time can cause lower fatalities. We believe that these potential explanatory variables should be included in the subsequent modeling work.

# (15 points) Preliminary Model

```{asis, echo=FALSE}
Estimate a linear regression model of *totfatrte* on a set of dummy variables for the years 1981 through 2004 and interpret what you observe. In this section, you should address the following tasks: 

> Please see below the linear model summary and residual diagnostic analysis.
```

```{asis, echo=FALSE}
- Why is fitting a linear model a sensible starting place? 
- What does this model explain, and what do you find in this model? 
- Did driving become safer over this period? Please provide a detailed explanation.
- What, if any, are the limitation of this model. In answering this, please consider **at least**: 
    - Are the parameter estimates reliable, unbiased estimates of the truth? Or, are they biased due to the way that the data is structured?
- Are the uncertainty estimate reliable, unbiased estimates of sampling based variability? Or, are they biased due to the way that the data is structured?
```

> As a start, fitting a linear model helps identify significant explanatory variables and evaluate whether the linear relationship exists and how strong the linear relationship is. See Appendix 1.5 the "Prelim" column for the model summary results.

```{r simple-linear-model, results='hide'}
mod.lm1 <- lm((total_fatalities_rate) ~ year_of_observation, data = df)
```

```{r, results='hide'}
coeftest(mod.lm1, vcov. = vcovHC, type = "HC1")
```

```{r lm-residuals, echo=FALSE, fig.align='center', out.width="50%", fig.cap="Linear Model Residuals"}
forecast::checkresiduals(mod.lm1)
```

> We expected the total fatalities rate to decline over the years given our EDA observations. This model shows whether a given year has a linear relationship with total fatalities rate, with 1980 as the base year in the linear model. There is strong statistical evidence that all the years (except for 1981) are negatively related to total fatalities rate at the significance level of 0. The negative relationship aligns with the decreasing trend observed in EDA. Year 1981 is not a significant variable in this model. 

> As noted in our EDA work, the average total fatalities rate was 24% in 1980 and by 2004 the average total fatalities decreased down to 16%. This linear model shows statistically significant relationship between time and total fatalities rates. However, based on our expericence, time by itself does not fully explain the changes in the fatalities rates. Many other factors (e.g. changes in law, economics and demographics, and technology) could explain the changes in total fatalities rates over the period, as noted in the summary notes of the above EDA section. This dataset is structured by state and by year with many other factors. This introduces omitted variable bias. 

> Model residuals have large fluctuations for each year and are not constant with zero mean expectation. This violates one of the key classical linear model assumptions - linear conditional mean. The parameter estimates are biased and not reliable. The distribution of residuals is not normal. variance is not constant (heteroskedasticity issues). The Breusch-Godfrey test for serial correlation has a p-value much less than 0.05, suggesting serial correlation. The ACF plot shows significant positive autocorrelation of many lags. With positive serial correlation, the standard errors/uncertainty estimates are understated and the statistical inferences are not reliable. Because of this, we used heteroskedasticity-robust standard errors which are notably higher than the normal standard errors.
   
# (15 points) Expanded Model 
```{asis, echo=FALSE}

Expand the **Preliminary Model** by adding variables related to the following concepts: 

- Blood alcohol levels 
- Per se laws
- Primary seat belt laws (Note that if a law was enacted sometime within a year the fraction of the year is recorded in place of the zero-one indicator.)
- Secondary seat belt laws 
- Speed limits faster than 70 
- Graduated drivers licenses 
- Percent of the population between 14 and 24 years old
- Unemployment rate
- Vehicle miles driven per capita. 

If it is appropriate, include transformations of these variables. Please carefully explain carefully your rationale, which should be based on your EDA, behind any transformation you made. If no transformation is made, explain why transformation is not needed.
```

> In the EDA section, we processed data and examined the histograms of continuous variables. We noted right-skewness in the distribution and applied the log transformation to total_fatalities_rate, unemployment_rate, and vehicle miles per capita to normalize the distribution. 

> In the summary notes of our EDA section, we identified the key variables that can explain total fatalities rate, which align with the variables that we will use in the expanded model. See the model results in Table 1 and Appendix 1.5 in the sections below.

```{r}
mod.lm2 <- lm(
  log(total_fatalities_rate) ~ year_of_observation
    + factor(blood_alcohol_limit)
    + factor(per_se_laws)
    + factor(seatbelt)
    + factor(speed_limit_70plus)
    + factor(graduated_drivers_license_law)
    + pct_population_14_to_24
    + unemployment_rate_log
    + vehicle_miles_per_capita_log,
  data = df
)
```

> Parameter estimates for all years are negative, similar to the preliminary model. However, a few added variables have unintuitive signs and are not sensible, for example: seatbelt 2 (secondary seatbelt) has a positive coefficient compared to no seatbelt (even though unsignificant).

> Figure 8 shows the model residuals analysis.

```{r expanded-lm-residuals, echo=FALSE, fig.align='center', out.width="50%", fig.cap="Expanded Linear Model Residuals"}
forecast::checkresiduals(mod.lm2)
```

> The model residuals improved over the prelimary linear model, but still have a lot of variance fluctuations. Breusch-Godfrey test and the ACF plot both indicated serial correlation issues. Standard error estimates and statistical inferences are not reliable. We need to use robust standard errors.

```{r, results='hide'}
# use the robust standard errors
coeftest(mod.lm2, vcov. = vcovHC, type = "HC1")
```

> We evaluated three law variables below. 

- How are the blood alcohol variables defined? Interpret the coefficients that you estimate for this concept. 

> Every state in the United States has laws making it illegal to operate a motor vehicle while under the influence of alcohol. In each state, the blood alcohol content (BAC) has a legal limit of 0.08% or 0.1% for ordinary, non-commercial vehicles. 

> blood_alcohol_limit of 0.1 is the base value in this model. The estimated coefficient on *blood alcohol limit 0.08* is -0.045 and significantly different from zero at 5%. The interpretation is: holding other factors constant, changing the legal blood alcohol limit from 0.10% to 0.08% will cause the traffic fatalities rate to be 4.4% lower  (or 0.956 times as big as the original fatalities rate). For example, if the fatalities rate with 0.1% blood alcohol limit was 20%, the estimated fatalities rate would be 19.12% with 0.08% blood alcohol limit.

```{r, echo=FALSE}
# coefficient of blood_alcohol_limit0.08
blood_alcohol_limit0.08_coef <- mod.lm2$coefficients[26]

# The effect on the total fatalities rate, if changing blood_alcohol_limit from 0.1% to 0.08%
# Since the total fatalities rates are log-transformed
# we will take the exponent of the coefficient to get the effect
round(exp(blood_alcohol_limit0.08_coef) - 1, 4)
```

- Do *per se laws* have a negative effect on the fatality rate? 

> Yes, the estimated coefficient on *per se laws* is -0.022 and aligns with our expectation. However, this coefficient is not significantly different from zero in this model. The interpretation is that holding other factors constant, changing the per se laws from no (0) to yes (1) will cause traffic fatalities rate to decrease by 2.17% (or 0.9783 times as big as the original fatalities rate). However, this effect is not statistically significant.

```{r, echo=FALSE}
# coefficient of per_se_laws
per_se_laws_coef <- mod.lm2$coefficients[27]

# The effect on the total fatalities rate, if per_se_laws from 0 to 1
# Since the response variable is the log form of the total fatalities rate,
# we will take the exponent of the coefficient to get the effect
round(exp(per_se_laws_coef) - 1, 4)
```


- Does having a primary seat belt law?

> Yes, the estimated coefficient on *primary seatbelt laws* (seatbelt1) is negative (-0.00067) and the negative sign aligns with our expectation. But it is not significantly different from zero in this model. This is a surprise, as we expect primary seatbelt law requiremnets to have a material impact on the total fatalities rate. Its interpretation is that changing the seatbelt law from no requirement to primary requirement will cause traffic fatalities rate to decrease marginally (0.067%), not statistically or practically significant. 

```{r, echo=FALSE}
# coefficient of primary seat belt
seatbelt1_coef <- mod.lm2$coefficients[29]

# The effect on the total fatalities rate, if changing seatbelt law from none to primary
# Since the response variable is the log form of the total fatalities rate,
# we will take the exponent of the coefficient to get the effect
exp(seatbelt1_coef) - 1
```

# (15 points) State-Level Fixed Effects 

Re-estimate the **Expanded Model** using fixed effects at the state level. 

> We used the same set of variables in the **Expanded Model** and added factor(state) to account for time-invariant fixed effects in the State-level fixed effects model.

```{r, results='hide', echo=TRUE}
# estimate the fixed effects regression with plm()
mod.fe <- plm(
  log(total_fatalities_rate) ~ year_of_observation
    + factor(blood_alcohol_limit)
    + factor(per_se_laws)
    + factor(seatbelt)
    + factor(speed_limit_70plus)
    + factor(graduated_drivers_license_law)
    + pct_population_14_to_24
    + unemployment_rate_log
    + vehicle_miles_per_capita_log
    + factor(state),
  data = df,
  index = c("state", "year_of_observation"),
  model = "within",
  effect = "individual"
)
```

```{r, results='hide'}
coeftest(mod.fe, vcov. = vcovHC, type = "HC1")
```

- What do you estimate for coefficients on the blood alcohol variables? How do the coefficients on the blood alcohol variables change, if at all? 

> In this State-level fixed effects model, the estimated coefficient on blood alcohol limit 0.08 is -0.0048883, much less negative than the estimated coefficient of -0.045 in the expanded model. 

> Additionally, the expanded linear model (mod.lm2) showed that this variable is statistically significant, but the State-level fixed effects model shows that this variable's coefficient is not significantly different from zero using the robust standard errors.

> Its interpretation is that changing the blood alcohol limit from 0.10% to 0.08% causes traffic fatalities rate to decrease marginally by 0.49%, which is less than the 4.4% decrease estimated by the expanded liniear model. 

```{r, echo=FALSE}
# coefficient of blood_alcohol_limit0.08
blood_alcohol_limit0.08_coef_fe <- mod.fe$coefficients[25]

# The effect on the total fatalities rate, if changing blood_alcohol_limit from 0.1% to 0.08%
# Since the response variable is the log form of the total fatalities rate,
# we will take the exponent of the coefficient to get the effect
round(exp(blood_alcohol_limit0.08_coef_fe) - 1, 4)
```

- What do you estimate for coefficients on per se laws? How do the coefficients on per se laws change, if at all? 

> The estimated coefficient on *per se laws* is -0.055 and significantly different from zero at 0% significance level, using the robust standard errors. In the prior expanded linear model (mod.lm2), this variable has a coefficient of -0.022 and was not statistically significant.

> Its interpretation is that change per se laws from no (0) to yes (1) will cause traffic fatalities rate to decrease by 5.4%. This is significant both statistically and practically. Compared to the expanded model, the impact magnitude of per se laws estimated by the fixed effects model is much larger and is sensible.

```{r, results='hide', echo=FALSE}
# coefficient of per_se_laws
per_se_laws_coef_fe <- mod.fe$coefficients[26]

# The effect on the total fatalities rate, if per_se_laws from 0 to 1
# Since the response variable is the log form of the total fatalities rate,
# we will take the exponent of the coefficient to get the effect
round(exp(per_se_laws_coef_fe) - 1, 4)
```


- What do you estimate for coefficients on primary seat-belt laws? How do the coefficients on primary seatbelt laws change, if at all? 

> The estimated coefficient on *primary seatbelt laws* is -0.041 and has strong evidence that it is difference from zero at 0.05% significance level (using standard errors) but not significant using the robust standard errors. The prior expanded model (mod.lm2) estimated a much less coefficient.

> Its interpretation is that chaning seatbelt law requirement from none to primary will cause traffic fatalities rate to decrease by 4.1%. This effect makes larger than the marginal effect estimated by the expanded model.  

```{r, results='hide', echo=FALSE}
# coefficient of primary seat belt
seatbelt1_coef_fe <- mod.fe$coefficients[28]

# The effect on the total fatalities rate, if changing seatbelt law from none to primary
# Since the response variable is the log form of the total fatalities rate,
# we will take the exponent of the coefficient to get the effect
round(exp(seatbelt1_coef_fe) - 1, 4)
```


Which set of estimates do you think is more reliable? Why do you think this?

```{r fe-residuals, echo=FALSE, fig.align='center', out.width="50%", fig.cap="Fixed-Effect Model Residuals"}
forecast::checkresiduals(mod.fe)
```

> For the State-level Fixed Effects models, the "within" setting eliminated the omitted variable bias for time-invariant fixed effects and improved the model performance over the linear models.  The linear models have omitted variable bias which caused the parameter estimates to be biased. 

> We performed residuals diagnostic analysis for all models. For the State-level Fixed Effects model, the model residuals have a conditional mean around zero, with a normal distribution. Since this model uses the within setting to fit a fixed effect model at the state level, the effect of unobserved individual heterogeneity is eliminated. The conditinoal mean of zero assumption is met. 

> Variance fluctulations are much improved compared to the linear models. The ACF plot for the model residuals shows autocorrelation but decays quickly (this is much improved compared to the significant but non-decaying autocorrelation in the expanded model). We used the robust standard errors to account for the group heteroskedasticity and serial correlation by allowing correlation across time in groups.  

> Based on the residuals analysis, the State-level Fixed Effects model will generate more reliable estimates than the linear models. Also, the parameter estimates' sign and magnitude in the State-level Fixed Effects model make much sense than the expanded model, based on our EDA work and our experience. 

- What assumptions are needed in each of these models?

> Fixed Effect Model Assumptions:

> 1) **Linearity**: the model is linear in parameters;  2) **i.i.d.** : The observations are independent across individual states but not necessarily across time. This is guaranteed through random sampling.  3) **Indentifiability**: the regressors, including a constant, are not perfectly collinear, and all regressors (but the constant) have non-zero variance and not too many extreme values.  4) **Zero conditional means (strict exogeneity)**  

- Are these assumptions reasonable in the current context?

> 1) **Linearity**: This assumption is met since each of the explanatory variable is shown to have a linear relationship with the response variable in the models. 

> 2)  **i.i.d.** : This assumption is met since the cross-sectional variables measured across state/year on a randomized population. Since we have 48 different states, we assume that each state has independent legislative procedures and has the state-specific economics and demographic situations. 

> 3) **Indentifiability**: In our EDA work, we excluded the variables that have perfect collinearity. Also any variables with perfect collinearity would be dropped out from the model. So this condition is met. Using robust standard error on the fitted model, each explanatory variables resulted in a less than 1 standard deviation, which, upon raising it to the power of two, resulted in low non-zero variance. This suggests there are not too many extreme values in the explanatory variables. Therefore, this condition is met.

> 4) **Zero conditional means (strict exogeneity)**: Since we're using within estimator to fit a fixed effect model on the state, the effect of unobserved individual heterogeneity is eliminated. Thus this condition is met by the State-level Fixed Effect Model. This condition is not met by the two linear models.


# (10 points) Consider a Random Effects Model 
```{asis, echo=FALSE}
Instead of estimating a fixed effects model, should you have estimated a random effects model?
```

- Please state the assumptions of a random effects model, and evaluate whether these assumptions are met in the data. 

> Random Effect Model Assumptions: All assumptions under Fixed Effect model. Additionally, the unobservbed effect term $a_{i}$ is independent of all explanatory variables in all the time periods.

- If the assumptions are, in fact, met in the data, then estimate a random effects model and interpret the coefficients of this model. Comment on how, if at all, the estimates from this model have changed compared to the fixed effects model. 

> The first set of assumption is already met under the fixed effect analsysis. To test for the last assumption, we will perform Hausman Test for Fixed vs. Random Effects. In the null hypothesis, both estimates are consistent but only the random effects model estimates are efficient (minimum variance). The alternative hypothes is that only the coefficients of the fixed effects model are consistent, and the coefficients of the random effects model are not consistent. The alternative hypothesis means that there is correlation between residuals and predictors. If Hausman test p-value is less than the significance level, we reject the null hypothesis and deem that the fixed-effects model should be preferred instead of a random-effects model.

> Our test result below shows a p-value of 2.649e-7, which is significantly lower than the 95% confidence level, alpha = 0.05. Thus, there are sufficient evidence to reject the null hypothesis that a random effects model is appropriate, suggesting that we should use the fixed effect models. The random effects model is not likely to be consistent in this case, which means that the paramater estimates won't get closer to the the true parameter values even with a large sample size. Based on this result, we don't think it is appropriate to use the random effects model for this dataset.

```{r random-effect-model estimate, results='hide'}
re.model <- plm(
  log(total_fatalities_rate) ~ year_of_observation
    + factor(blood_alcohol_limit)
    + factor(per_se_laws)
    + factor(seatbelt)
    + factor(speed_limit_70plus)
    + factor(graduated_drivers_license_law)
    + pct_population_14_to_24
    + unemployment_rate_log
    + vehicle_miles_per_capita_log,
  data = df,
  index = c("state", "year_of_observation"),
  model = "random"
)
# Hausman test below shows a p-value of 2.649e-7
phtest(mod.fe, re.model)
# show partial coefficient of state-level time-invariant fixed effects
ranef(re.model)
```

> While we don't think it is appropriate to use the random effect model, we included the random effect model in the comparison below for information only. We decided to remove the date effect here to save spaces, please check Appendix 5.1 for the full models including the dates

```{r, expand-fixed-random-comparison, results='asis', message=FALSE, warning=FALSE, echo=FALSE}
# calculate the robust standard errors for the models

cov_pre <- vcovHC(mod.lm1, type = "HC1")
robust_se_pre <- sqrt(diag(cov_pre))

cov_exp <- vcovHC(mod.lm2, type = "HC1")
robust_se_exp <- sqrt(diag(cov_exp))

cov_fe <- vcovHC(mod.fe, type = "HC1")
robust_se_fe <- sqrt(diag(cov_fe))

cov_rm <- vcovHC(re.model, type = "HC1")
robust_se_rm <- sqrt(diag(cov_rm))

# compare the three models, show robust standard errors in the comparison
stargazer(mod.lm1, mod.lm2, mod.fe, re.model,
  type = "latex",
  omit.stat = c("adj.rsq", "f"),
  se = list(robust_se_exp, robust_se_fe, robust_se_rm),
  omit = c("year_of_observation"),
  column.labels = c("Prelim", "Expand", "Fixed Effects", "Random Effects"),
  title = "Estimated Models on Total Fatality Rate - Exclude Year Variables (See Appendix 1.5 for full list)",
  column.sep.width = "-10pt",
  header = FALSE, # to get rid of r package output text,
  single.row = TRUE, # to put coefficients and standard errors on same line
  no.space = TRUE # to remove the spaces after each line of coefficients
)
```

- If the assumptions are **not** met, then do not estimate the data. But, also comment on what the consequences would be if you were to *inappropriately* estimate a random effects model. Would your coefficient estimates be biased or not? Would your standard error estimates be biased or not? Or, would there be some other problem that might arise?

> Since the null hypothesis for the Hausman test is rejected, the last assumption is not met. Thus we should not use the random effect model to estimate the coefficients since the estimates will not likely to be consistent.

> The random effects model assumes the individual unobserved heterogeneity is uncorrelated with the independent variables. However, the Hausman test shows strong evidence that correlation exists. Therefore, using the random effects model for this dataset could produce bias in parameters estimates. This bias does not arise in the fixed effects model, because the fixed effects model assumes that the time-invariant fixed effects can be correlated with the other variables. Fixed effects models remove the effect of time-invariant variables so we can assess the net effect of the other predictors on the response variable (total traffic fatalities rate).

> Given the Hausman test result, the random effects model is also not likely to be efficient, which means its variance would not be minimum or stable causing the standard errors to be biased and test statistics to be not reliable. 

> From a practical perspective, fixed effects models are relatively straighforward. Random effects models require additional mathematical assumptions with added-complexity, which strenthens the argument of not using the random effects model in this case. 

> This is especially true because there is no time invariant explanatory variables in our model, so the biggest advantage of the random effects model (being able to estimate the impacts of time-invariant explanatory variables)  is not applicable here.

# (10 points) Model Forecasts 
```{asis, echo=FALSE}

The COVID-19 pandemic dramatically changed patterns of driving. Find data (and include this data in your analysis, here) that includes some measure of vehicle miles driven in the US. Your data should at least cover the period from January 2018 to as current as possible. With this data, produce the following statements: 

- Comparing monthly miles driven in 2018 to the same months during the pandemic: 
  - What month demonstrated the largest decrease in driving? How much, in percentage terms, lower was this driving? 
  - What month demonstrated the largest increase in driving? How much, in percentage terms, higher was this driving? 
```
  
> We collected the United States Motor Vehicle Miles Traveled Total (Millions) data from the US Department of Transformation from Jan 1980 to January 2023. This data is available at here [https://www.fhwa.dot.gov/policyinformation/statistics/2021/vm202.cfm]. We also downloaded related Bloomberg data (under ticker: VMTDVCLE Index) and saved it in the file `data/VMTDVCLE.csv`.


```{r monthly-miles-driven-covid, fig.align='center', out.width="65%", echo=FALSE, fig.cap="Comparison of Origianl and New Vehicle Miles"}
# load the monthly new panel data
vehicle_miles_month <- read.csv("./data/VMTDVCLE.csv",
  sep = ",",
  skip = 5
) %>%
  as_tibble() %>%
  mutate(
    vehicle_miles = PX_LAST / 1000, # convert to billions
    Date = as.Date(Date, format = "%m/%d/%Y"),
    year = year(Date),
    month = month(Date)
  ) %>%
  select(year, month, vehicle_miles) %>%
  arrange(year, month)

# create the annual data by grouping the new monthly data by year
vehicle_miles_annual <- vehicle_miles_month %>%
  filter(year < 2023) %>% # remove the 2023 data since it's not complete
  group_by(year) %>%
  summarise(total = sum(vehicle_miles))

# create the annual data by grouping the original data by year
df_annual <- df %>%
  group_by(year) %>%
  summarise(total = sum(vehicle_miles))

# calculate the avg difference between the two data
#
combined_mile_annual <- vehicle_miles_annual %>%
  filter(year <= 2004) %>%
  dplyr::rename(
    "new" = "total",
    "new_year" = "year"
  ) %>%
  # rename(new = total, new_year = year) %>%
  cbind(df_annual) %>%
  mutate(diff = (new - total) / total)

avg_diff <- round(mean(combined_mile_annual$diff) * 100, 1)


# visualize the new vs the old vehicle miles data
ggplot() +
  geom_line(
    data = vehicle_miles_annual,
    aes(x = year, y = total, color = "New Dataset Annual")
  ) +
  geom_line(
    data = df_annual,
    aes(x = year, y = total, color = "Original Dataset Annual")
  ) +
  labs(
    title = "US Vehicle Miles Traveled (billions)",
    subtitle = paste0(
      "New Data(Bloomberg) vs Original Data: Avg. difference by year is ~",
      avg_diff, "%"
    ),
    x = "Year",
    y = "Vehicle Miles Traveled (billions)"
  )
```


> Figure 6 shows that new vehicle milage data is similar to the old data. The average difference between the two data is marginal (0.8%). This difference is expected as the new vehicle miles data from Bloomberg includes 51 states, while the old data only includes 48 states. We believe that the new data is appropriate to use in our analysis.

> The largest decrease in driving during COVID bust is in April 2020 (-39.1% compared with April 2018), and the largest increase in driving post-COVID boom is in September 2022 (+5.0% compared with January 2018). We didn't consider January and February in 2020 because COVID hasn't started yet then.

```{r monthly-miles-driven-covid-change, fig.align='center', out.width="45%", echo=FALSE}
vehicle_miles_month %>%
  filter(year >= 2018 & year < 2023) %>%
  group_by(month) %>%
  summarise(
    year = year,
    chg_vs_2018 = round((vehicle_miles / vehicle_miles[year == 2018] - 1) * 100, 1)
    # vehicle_miles_yoy = round((vehicle_miles / lag(vehicle_miles) - 1) * 100, 1) # yoy
  ) %>%
  filter(year > 2018) %>% # remove the first row
  spread(year, chg_vs_2018) %>%
  knitr::kable()
```


```{asis, echo=FALSE}

Now, use these changes in driving to make forecasts from your models. 
- Suppose that the number of miles driven per capita, increased by as much as the COVID boom. Using the FE estimates, what would the consequences be on the number of traffic fatalities? Please interpret the estimate.

- Suppose that the number of miles driven per capita, decreased by as much as the COVID bust. Using the FE estimates, what would the consequences be on the number of traffic fatalities? Please interpret the estimate.
```

```{r estimate-the-impact-of-covid}
beta <- tail(mod.fe$coefficients, 1)
(1 + 0.05)^beta - 1
(1 - 0.391)^beta - 1
```

> Holding other factors constant, if vehicle miles per capita increases by 5.0%, the traffic fatalities rate would increase by 0.83% (or become 99.17% as large), compared with before COVID levels.

> Holding other factors constant, if vehicle miles per capita decreases by 39.1%, the traffic fatalities rate would decrease by 8.13% (or become 92.87% as large), compared with before COVID levels. Althought we should be cautious here: the change in miles driven per capita is very large, so the log transformation interpretation might not generate a good estimation in this case.

# (5 points) Evaluate Error 

If there were serial correlation or heteroskedasticity in the idiosyncratic errors of the model, what would be the consequences on the estimators and their standard errors? Is there any serial correlation or heteroskedasticity? 

```{r heteroskedasticity-test, results=FALSE}
pcdtest(mod.fe, test = "lm")
```

```{r serial-correlation-test, results=FALSE}
pbgtest(mod.fe, order = 2)
```

> If there is serial correlation or heteroskedasticity in the idiosyncratic errors of the model, the standard errors or uncertainty of the estimators will be underestimated and statistical inferences are not reliable. However, serial correlation does not cause bias in the regression coefficient estimates of the estimators. 

> In this case, when we apply the Breusch Pagan Test on homoskedasticity to the FE model, we obtained a p-value of 2.2e-16, which is significantly less than 0.05, suggesting that there is sign of heteroskedasticity from the idiosyncratic errors. We also performed the Breusch-Godfrey test for serial correlation and obtained a p-value of 2.2e-16, which is significantly less than 0.05, suggesting that we have do have serially correlation in idiosyncratic errors. In the ACF plot, we can also visually detect the autocorrelation in the model residuals. 

> Therefore, the FE model has serial correlation issues and standard errors are underestimated. We should use the robust standard errors to address heteroskedasticity and serial correlation issues by allowing correlation across time in groups.

\newpage

# Appendix

> **Appendix 1.1**

```{r data-processing2, ref.label='data-processing', echo=TRUE}
```

\newpage
> **Appendix 1.2**

```{r clean-data2, ref.label='clean-data', echo=TRUE}

```
\newpage
> **Appendix 1.3**

```{r, categorical-variables2, ref.label='categorical-variables', echo=TRUE}

```
\newpage
> **Appendix 1.4**

```{r, total_fatalities_rate-by-state2, ref.label='total_fatalities_rate-by-state', echo=TRUE}

```
\newpage
> **Appendix 1.5**

```{r, expand-fixed-random-comparison-full,header=FALSE, results='asis', message=FALSE, warning=FALSE, echo=FALSE}
# Appendix 1.5
# calculate the robust standard errors for the models
cov_pre <- vcovHC(mod.lm1, type = "HC1")
robust_se_pre <- sqrt(diag(cov_pre))

cov_exp <- vcovHC(mod.lm2, type = "HC1")
robust_se_exp <- sqrt(diag(cov_exp))

cov_fe <- vcovHC(mod.fe, type = "HC1")
robust_se_fe <- sqrt(diag(cov_fe))

cov_rm <- vcovHC(re.model, type = "HC1")
robust_se_rm <- sqrt(diag(cov_rm))

# compare the three models, show robust standard errors in the comparison
stargazer(mod.lm1, mod.lm2, mod.fe, re.model,
  type = "latex",
  omit.stat = c("adj.rsq", "f"),
  se = list(robust_se_pre, robust_se_exp, robust_se_fe, robust_se_rm),
  column.labels = c("Prelim", "Expand", "Fixed Effects", "Random Effects"),
  title = "Estimated Models on Total Fatality Rate",
  column.sep.width = "-10pt",
  header = FALSE, # to get rid of r package output text,
  single.row = TRUE, # to put coefficients and standard errors on same line
  no.space = TRUE # to remove the spaces after each line of coefficients
)
```

```{r warning=FALSE, echo=FALSE, include=FALSE}
par(mfrow = c(2, 2))
plot(mod.lm2)
```


```{r warning=FALSE, echo=FALSE, include=FALSE}
forecast::checkresiduals(mod.fe)
```

```{r, warning=FALSE, include=FALSE, echo=FALSE}
forecast::checkresiduals(re.model)
```

```{r check random effects, include=FALSE, echo=FALSE}
re <- data.frame(ranef(re.model))
re
```
