---
title: 'Lab 3: Panel Models'
subtitle: 'US Traffic Fatalities: 1980 - 2004'
output: 
  bookdown::pdf_document2: default
---

```{r load packages, echo=FALSE, message=FALSE, warning=FALSE}
library(plm)
library(lmtest)
library(stargazer)
library(tidyverse)
library(patchwork)
library(magrittr)
library(dplyr)

if (!"ggthemes" %in% rownames(installed.packages())) {
  install.packages("ggthemes")
}
library(ggthemes)

if (!"scales" %in% rownames(installed.packages())) {
  install.packages("scales")
}
library(scales)

if (!"mgcv" %in% rownames(installed.packages())) {
  install.packages("mgcv")
}
library(mgcv)

if (!"reshape2" %in% rownames(installed.packages())) {
  install.packages("reshape2")
}
library(reshape2)
library(lubridate)
```


# U.S. traffic fatalities: 1980-2004

In this lab, we are asking you to answer the following **causal** question: 

> **"Do changes in traffic laws affect traffic fatalities?"**  

To answer this question, please complete the tasks specified below using the data provided in `data/driving.Rdata`. This data includes 25 years of data that cover changes in various state drunk driving, seat belt, and speed limit laws. 

Specifically, this data set contains data for the 48 continental U.S. states from 1980 through 2004. Various driving laws are indicated in the data set, such as the alcohol level at which drivers are considered legally intoxicated. There are also indicators for “per se” laws—where licenses can be revoked without a trial—and seat belt laws. A few economics and demographic variables are also included. The description of the each of the variables in the dataset is also provided in the dataset. 

```{r load data, echo = TRUE}
load(file = "./data/driving.RData")

## please comment these calls in your work

# glimpse(data)
# desc
```


# (30 points, total) Build and Describe the Data 

1. (5 points) Load the data and produce useful features. Specifically: 
    - Produce a new variable, called `speed_limit` that re-encodes the data that is in `sl55`, `sl65`, `sl70`, `sl75`, and `slnone`; 
    - Produce a new variable, called `year_of_observation` that re-encodes the data that is in `d80`, `d81`, ... , `d04`. 
    - Produce a new variable for each of the other variables that are one-hot encoded (i.e. `bac*` variable series). 
    - Rename these variables to sensible names that are legible to a reader of your analysis. For example, the dependent variable as provided is called, `totfatrte`. Pick something more sensible, like, `total_fatalities_rate`. There are few enough of these variables to change, that you should change them for all the variables in the data. (You will thank yourself later.)
```{r, warning=FALSE, message=FALSE}
# produce new variables - year_of_observation, speed_limit, speed_limit_70plus, blood_alcohol_limit
# 
df <- data %>%
  mutate(state = factor(state)) %>%
  rowwise() %>%
  # speed_limit
  mutate(
    speed_limit_70plus = factor(sl70plus),
    speed_limit = parse_number(
      colnames(
        select(data, starts_with("sl"))
      )[which.max(c_across(starts_with("sl")))],
      na = "slnone"
    ),
  ) %>%
  select(-starts_with("sl")) %>%
  mutate(year_of_observation = factor(year)) %>% # year_of_observation
  select(-starts_with("d")) %>%
  mutate(blood_alcohol_limit = factor(parse_number(
    colnames(
      select(data, starts_with("bac"))
    )[which.max(c_across(starts_with("bac")))]
  ) / 100)) %>% # blood_alcohol_limit
  select(-starts_with("bac")) %>%
  mutate(
    seatbelt = factor(seatbelt), # 'seatbelt' categorizes primary or secondary
    speed_limit_70plus = ifelse(speed_limit == 55 | speed_limit == 65, 0, 1)
  ) %>%
  select(-starts_with("sb"))

# rename the variables to sensible names
df <- df %>%
  dplyr::rename(
    "total_fatalities_rate" = "totfatrte",
    "minimum_drinking_age" = "minage",
    "zero_tolerance_law" = "zerotol",
    "graduated_drivers_license_law" = "gdl",
    "per_se_laws" = "perse",
    "total_traffic_fatalities" = "totfat",
    "total_nighttime_fatalities" = "nghtfat",
    "total_weekend_fatalities" = "wkndfat",
    "total_fatalities_per_100_million_miles" = "totfatpvm",
    "nighttime_fatalities_per_100_million_miles" = "nghtfatpvm",
    "weekend_fatalities_per_100_million_miles" = "wkndfatpvm",
    "nighttime_fatalities_rate" = "nghtfatrte",
    "weekend_fatalities_rate" = "wkndfatrte",
    "vehicle_miles" = "vehicmiles",
    "unemployment_rate" = "unem",
    "pct_population_14_to_24" = "perc14_24",
    "vehicle_miles_per_capita" = "vehicmilespc"
  ) %>%
  select(
    year_of_observation,
    state,
    year,
    # response variables
    total_fatalities_rate,
    nighttime_fatalities_rate,
    weekend_fatalities_rate,
    total_traffic_fatalities,
    total_nighttime_fatalities,
    total_weekend_fatalities,
    total_fatalities_per_100_million_miles,
    nighttime_fatalities_per_100_million_miles,
    weekend_fatalities_per_100_million_miles,
    # potential explanatory variables
    seatbelt,
    zero_tolerance_law,
    graduated_drivers_license_law,
    per_se_laws,
    minimum_drinking_age,
    speed_limit_70plus,
    speed_limit,
    blood_alcohol_limit,
    vehicle_miles,
    vehicle_miles_per_capita,
    # econ and demographic variables
    statepop,
    unemployment_rate,
    pct_population_14_to_24, vehicle_miles
  ) # keep the similar variables together

# check the data
# df %>% glimpse()
```

```{r echo=FALSE}
check_sl65 <- data %>% 
  filter(sl65>0 & sl65<1) %>% 
  select(sl55,sl65,sl70,sl75,slnone) %>% 
  mutate(sum_sl = sum(sl55,sl65,sl70,sl75,slnone)) 
head(check_sl65, 2)
```

> Several notes about our data processing: 

> 1) As noted in the above 2 examples, the original dataset has some percentage values in the sl55/sl65/sl70/s75/slone columns. We expect these columnes to be binary (0, 1). In our data processing, we classified the category based on the field with the max value.  For example, we classified the first example as sl55, and the second example as sl70.


> 2) speed_limit_70plus column has values that are not 0 or 1. We reclassified these values to 0 or 1, based on speed_limit column. We als reclassified the states with no speed limit as 1 for this variable at this stage.

```{r data-diagnose-and-cleanup categorical fields}
# check the values of these fields
# df$zero_tolerance_law %>%
#   table(useNA = "ifany") %>%
#   as.data.frame()
# 
# df$graduated_drivers_license_law %>%
#   table(useNA = "ifany") %>%
#   as.data.frame()
# 
# df$per_se_laws %>%
#   table(useNA = "ifany") %>%
#   as.data.frame()

# make binary variables
df <- df %>%
  mutate(
    zero_tolerance_law = ifelse(
      zero_tolerance_law == 0 | zero_tolerance_law == 1, zero_tolerance_law, 1
    ),
    graduated_drivers_license_law = ifelse(
      graduated_drivers_license_law == 0 | graduated_drivers_license_law == 1,
      graduated_drivers_license_law,
      1
    ),
    per_se_laws = ifelse(
      per_se_laws == 0 | per_se_laws == 1, per_se_laws, 1
    )
  )
```

> 3) We observed non-binary values in the following columns: zero_tolerance_law, graduated_drivers_license_law, per_se_laws. Since we expect these columns to have binary values (0,1) given the definition, we decided to treat all non-zero values as 1 and make it a binary variable.

```{r minimum-drinking-age}
#check minimum_drinking_age 
# df$minimum_drinking_age %>%
#   table(useNA = "ifany") %>%
#   as.data.frame()

df <- df %>%
  mutate(
    minimum_drinking_age = round(minimum_drinking_age, 0)
  )

```

> 4) We noticed that the minimum_drinking_age column has values that are not integers. We decided to round them to the nearest integer.

```{r missing-speed-limit}

#check for na speed_limit rows
check_na_speed <- df %>%
  filter(is.na(speed_limit)) %>%
  select(state, year_of_observation, speed_limit, speed_limit_70plus)

head(check_na_speed,2)

# treat the na speed_limit rows for State 27
df <- df %>%
  mutate(
    speed_limit = ifelse(
      is.na(speed_limit) & state == 27, ifelse(
        year >= 1996 & year <= 1999, 85, 75
      ), speed_limit
    ),
    speed_limit_70plus = ifelse(
      is.na(speed_limit_70plus) & state == 27, 1, speed_limit_70plus
    )
  )
```

> 5) We observed that the $speed\_limit$ is not set for State 27, between 1996 to 2004. Our background research noted the fact that "for three years after the 1995 repeal of the increased 65 mph limit, Montana had a non-numeric "reasonable and prudent" speed limit during the daytime on most rural roads". But it doesn't mean there was no speed limit. 

> We decided to set the $speed\_limit$ to 85 for Montana between 1996 to 1999, given the legal case of [State v. Rudy Stanko (1998)](https://en.wikipedia.org/wiki/Speed_limits_in_the_United_States_by_jurisdiction), who got charged for speed of 85. Effective May 28, 1999, as a result of that decision, the Montana Legislature established a speed limit of 75 mph. So we set the $speed\_limit$ to 75 for Montana between 2000 to 2004.

2. (5 points) Provide a description of the basic structure of the dataset. What is this data? How, where, and when is it collected? Is the data generated through a survey or some other method? Is the data that is presented a sample from the population, or is it a *census* that represents the entire population? Minimally, this should include:
    - How is the our dependent variable of interest `total_fatalities_rate` defined? 
    
    > This data set is a balanced longitudinal dataset and contains traffic fatalities data for the 48 continental U.S. states from 1980 through 2004. For each year of observation, the dataset contains state-level cross sectional measurements of fatality count and rate. This data is collected and distributed by Jeffrey M. Wooldridge through this [link](https://github.com/JustinMShea/wooldridge). 
    
   > After our data processing work, the clean dataset (df) has 25 columns/fields which include:
    
    - Index variables: year_of_observation, state, year
    - 9 fatality variables: There are three measurements - fatality count, fatality count per 100M miles and fatality rate as defined as count per 100k population. These three measurements are provided for total, nighttime and weekend
    - 10 law and vehicle variables: 8 traffic laws indicators (seatbelt, zero_tolerance_law, graduated_drivers_license_law, per_se_laws, minimum_drinking_age, speed_limit_70plus, speed_limit, blood_alcohol_limit) and 2 driving variables (vehicle_miles, vehicle_miles_per_capita)
    - 3 Economics and demographic variables: statepop, unemployment_rate, pct_population_14_to_24
    
  > In this dataset, the total_fatalities_rate is defined as total fatalities per 100,000 population.	

    
3. (20 points) Conduct a very thorough EDA, which should include both graphical and tabular techniques, on the dataset, including both the dependent variable `total_fatalities_rate` and the potential explanatory variables. Minimally, this should include: 
  - How is the our dependent variable of interest `total_fatalities_rate` defined? 
  
  > In this dataset, the total_fatalities_rate is defined as total fatalities per 100,000 population.	   
  
  - What is the average of `total_fatalities_rate` in each of the years in the time period covered in this dataset? 
  
  > See below the table

```{r avg-fatality-by-year}
df_avg <- df %>%
  group_by(year_of_observation) %>%
  summarise(avg_total_fatalities_rate = mean(total_fatalities_rate))
# average fatality by year
years <- unique(df$year_of_observation)
avg_df <- data.frame(
  year = years,
  avg_fatality_rate = round(df_avg$avg_total_fatalities_rate, 2)
) %>%
  knitr::kable()
avg_df

# plot fatality by year
df_avg %>%
  ggplot(aes(year_of_observation, avg_total_fatalities_rate,
    label = round(avg_total_fatalities_rate, 2)
  )) +
  geom_point() +
  geom_text(hjust = 0, vjust = -0.5) +
  theme_bw() +
  labs(title = "Average Total Fatalities Rate by Year") +
  xlab("Year") +
  ylab("Average Total Fatalities Rate %")
```

> Next, check the distribution of the continuous variables  
  
```{r, warning=FALSE, message=FALSE}
df %>%
  select(
    total_fatalities_rate,
    nighttime_fatalities_rate,
    weekend_fatalities_rate,
    vehicle_miles,
    vehicle_miles_per_capita,
    statepop,
    unemployment_rate,
    pct_population_14_to_24
  ) %>%
  keep(is.numeric) %>%
  gather() %>%
  ggplot(aes(value)) +
  facet_wrap(~key, scales = "free") +
  geom_histogram()

# hist(df$pct_population_14_to_24)
# hist(log(df$pct_population_14_to_24))
```

> For the continuous variables, the distributions are right-skewed for most of these variables.

```{r}
df %>%
  select(
    seatbelt,
    zero_tolerance_law,
    graduated_drivers_license_law,
    per_se_laws,
    minimum_drinking_age,
    speed_limit_70plus,
    speed_limit,
    blood_alcohol_limit
  ) %>%
  gather() %>%
  ggplot(aes(value)) +
  facet_wrap(~key, scales = "free") +
  geom_bar()
```

> We observed the following distribution for these categorical variables
 - Blood alcohol limit: Most states have the limit of 0.1.
 - Minimum drinking age: Most states have 21.
 - Graduated drivers license law, most states have 0
 - Speed limit: Most states have less than 70 miles
 

```{r cross-sectional-plot, fig.height=6, fig.width=8}
# we do have 48 states -> missing states 2, 9, and 12.
# Thus, the ordering is a little bit off
df %>%
  ggplot(aes(year, total_fatalities_rate, color = state)) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm") +
  facet_wrap(~state) +
  theme_economist_white(gray_bg = FALSE) +
  theme(
    legend.position = "none",
    axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, size = 8),
    axis.text.y = element_text(size = 8),
    strip.text = element_text(size = 8)
  ) +
  scale_y_continuous() +
  xlab("State") +
  ylab("Total Fatalities Rate %")
```

> Most states have downward trends in total fatalities rate. Here we notcied that the states are not numerically order sequentially from 1 to 48. Rather, the data is showned to be missin state number 2, 9, and 12. Thus, the numbering of the states is a little confusing in the plot

```{r pairplot-on-feature-of-interest-continuous, fig.height=5, fig.width=6}
df %>%
  mutate(
    vehicle_miles_log = log(vehicle_miles),
    vehicle_miles_per_capita_log = log(vehicle_miles_per_capita),
    statepop_log = log(statepop),
    pct_population_14_to_24_log = log(pct_population_14_to_24),
    unemployment_rate_log = log(unemployment_rate)
  ) %>%
  select(
    total_fatalities_rate,
    nighttime_fatalities_rate,
    weekend_fatalities_rate,
    vehicle_miles,
    vehicle_miles_log,
    vehicle_miles_per_capita,
    vehicle_miles_per_capita_log,
    statepop,
    statepop_log,
    pct_population_14_to_24,
    pct_population_14_to_24_log,
    unemployment_rate,
    unemployment_rate_log
  ) %>%
  melt(id.vars = c("total_fatalities_rate")) %>%
  ggplot(aes(value, total_fatalities_rate, color = variable)) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm") +
  facet_wrap(~variable, scales = "free_x") +
  # theme_economist_white(gray_bg=F) +
  theme(
    legend.position = "none",
    axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, size = 8),
    axis.text.y = element_text(size = 8),
    strip.text = element_text(size = 8)
  ) +
  # scale_y_continuous(label=percent) +
  xlab("Continous Feature vs Total Fatalities Rate") +
  ylab("Total Fatality Rate")
```

> We conducted log transformation on the following variables: vehicle_miles, vehicle_miles_per_capita, statepop, pct_population_14_to_24, unemployment_rate. We visualized both the original and log transformed variables, and decided to use log transformation for our interpretation for vehicle_miles, vechicle_miles_per_capita, statepop and unemployment_rate.

> Total fatalities rate is positively correlated with unemployment_rate and percentage of population aged 14 through 24.

> Total fatalities rate is negatively correlated with vehicle miles and state population, but positively correlated with vehicle miles per capita. 

> We interpret this as population and vehicle miles are both increasing with time, so is the other driving forces of fatalities rate (quality of the car, technology of the car, road conditions, etc.), so the relationship between total fatalities rate vs vehicle miles and state population is potentially spurious, and thus inconsistent with our background knowlege.

> However, the relationship between total fatalities rate and vehicle miles per capita is positive, because as the density of the population increases, there is expected to be more severe traffic incidents that leads to higher fatalities rate. This is consistent with our background knowledge.

```{r save-log-transformation}
df <- df %>%
  mutate(
    vehicle_miles_log = log(vehicle_miles),
    vehicle_miles_per_capita_log = log(vehicle_miles_per_capita),
    statepop_log = log(statepop),
    unemployment_rate_log = log(unemployment_rate)
  )
```


```{r pairplot-on-feature-of-interest-discrete, fig.height=5, fig.width=6}
df %>%
  select(
    total_fatalities_rate,
    seatbelt,
    zero_tolerance_law,
    graduated_drivers_license_law,
    per_se_laws,
    minimum_drinking_age,
    speed_limit_70plus,
    speed_limit,
    blood_alcohol_limit
  ) %>%
  melt(id.vars = c("total_fatalities_rate")) %>%
  ggplot(aes(value, total_fatalities_rate)) +
  geom_boxplot(aes(fill = factor(value))) +
  coord_flip() +
  facet_wrap(~variable, scales = "free") +
  theme(
    legend.position = "none",
    axis.text.x = element_text(size = 8),
    axis.text.y = element_text(size = 8),
    strip.text = element_text(size = 8)
  ) +
  xlab("Discrete Feature vs Total Fatalities Rate") +
  ylab("Total Fatality Rate")
```


```{r compute-pearson-correlation-confusion-matrix, fig.height=8, fig.width=8}
df %>%
  mutate(seatbelt_num = as.numeric(seatbelt)) %>%
  select(
    total_fatalities_rate,
    nighttime_fatalities_rate,
    weekend_fatalities_rate,
    vehicle_miles_log,
    statepop_log,
    vehicle_miles_per_capita_log,
    unemployment_rate_log,
    pct_population_14_to_24,
  ) %>%
  mutate_all(as.numeric) %>%
  cor() %>%
  melt() %>%
  ggplot(aes(Var1, Var2, fill = value)) +
  geom_tile() +
  theme_economist_white(gray_bg = FALSE) +
  theme(
    legend.title = element_blank(),
    legend.text = element_text(size = 10),
    axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)
  ) +
  scale_fill_gradient2(
    low = "cornflowerblue", high = "coral", mid = "white",
    midpoint = 0, limit = c(-1, 1)
  )
```

> The state population and vehicle miles are almost perfectly correlated. So to avoid the colinearity problem, we will only use vehicle_miles_per_capita in our model, which is impacted both of them. 

### Summary Of EDA

> 1. clean up the dataframe to keep only variables we care;
> #TODO: as a team, we can write some observations, and rationalize which are the variables we end up using in the next session.
> 2. transform some variables via log 
> #TODO: later in the Expanded Model part: > A log transformation is applied to total_fatalities_rate and unemployment_rate because the skewed distribution needs to be normalized. 

> Based on our EDA using the boxplot, we noticed that the variables blood_alcohol_limit, per se laws, seatbelt, speed_limit_70plus, and  graduated_drivers_license_law affected the total_fatalities_rate differently at each different category. For the continous variables, pct_population_14_to_24, the log transformed unemployment_rate, and the log transformed vehicle_miles_per_capita, we noticed that there is a linear relationship between each of these two variables and the total_fatalities_rate. Before modeling, we applied a log transformation is applied to total_fatalities_rate, unemployment_rate, and vehicle_miles_per_capita because the skewed distribution needs to be normalized. 


As with every EDA this semester, the goal of this EDA is not to document your own process of discovery -- save that for an exploration notebook -- but instead it is to bring a reader that is new to the data to a full understanding of the important features of your data as quickly as possible. In order to do this, your EDA should include a detailed, orderly narrative description of what you want your reader to know. Do not include any output -- tables, plots, or statistics -- that you do not intend to write about.


# (15 points) Preliminary Model

Estimate a linear regression model of *totfatrte* on a set of dummy variables for the years 1981 through 2004 and interpret what you observe. In this section, you should address the following tasks: 
    
```{r simple-linear-model}
mod.lm1 <- lm((total_fatalities_rate) ~ year_of_observation, data = df)
summary(mod.lm1)

par(mfrow = c(2, 2))
plot(mod.lm1)
```

```{r}
coeftest(mod.lm1, vcov. = vcovHC, type = "HC1")
forecast::checkresiduals(mod.lm1)
```

- Why is fitting a linear model a sensible starting place? 
> Fitting a linear model helps identify significant explanatory variables and evaluate how strong the linear relationship is. 

- What does this model explain, and what do you find in this model? 
> This model explains whether a given year has a linear relationship with total fatalities rate. Based on the model, there is strong evidence that all the years except 1981 are related to total fatalities rate at the significance level of 0.

> This makes sense as we are using 1980 as the baseline year, so 1981 is the first year after 1980 and as a result, all the time-variant effects have not kicked in. 

- Did driving become safer over this period? Please provide a detailed explanation.
> In 1980, the average total fatalities rate was 24% and by 2004, the average total fatalities decreased down to 16%.

- What, if any, are the limitation of this model. In answering this, please consider **at least**: 
    - Are the parameter estimates reliable, unbiased estimates of the truth? Or, are they biased due to the way that the data is structured?
    
    > The parameter estimates are biased because the model recognizes year as the sole factor affecting total fatalities rate. This introduces omitted variable bias because we have not accounted for other factors such as economic laws and beer taxes that affect fatalities. 
    
    - Are the uncertainty estimate reliable, unbiased estimates of sampling based variability? Or, are they biased due to the way that the data is structured?
    
    > To evaluate uncertainty estimates, we look at the residuals plots below. The residuals vs fitted values form a horizontal band around the residual = 0 line, which means that error terms are equal. However, the residuals are fixed for a certain fitted value, forming vertical lines. The QQ plot is well-behaved except for the head and tail end residuals. This suggests that the uncertainty estimates are biased estimates of sampling based variability because the model residuals are separated by year.

# (15 points) Expanded Model 

Expand the **Preliminary Model** by adding variables related to the following concepts: 

- Blood alcohol levels 
- Per se laws
- Primary seat belt laws (Note that if a law was enacted sometime within a year the fraction of the year is recorded in place of the zero-one indicator.)
- Secondary seat belt laws 
- Speed limits faster than 70 
- Graduated drivers licenses 
- Percent of the population between 14 and 24 years old
- Unemployment rate
- Vehicle miles driven per capita. 

If it is appropriate, include transformations of these variables. Please carefully explain carefully your rationale, which should be based on your EDA, behind any transformation you made. If no transformation is made, explain why transformation is not needed.
> A log transformation is applied to total_fatalities_rate, unemployment_rate, and vehicle miles per capita because the skewed distribution needs to be normalized. 

```{r}
mod.lm2 <- lm(
  log(total_fatalities_rate) ~ year_of_observation
    + blood_alcohol_limit
    + per_se_laws
    + seatbelt
    + speed_limit_70plus
    + graduated_drivers_license_law
    + pct_population_14_to_24
    + unemployment_rate_log
    + vehicle_miles_per_capita_log,
  data = df
)
summary(mod.lm2)
par(mfrow = c(2, 2))
plot(mod.lm2)
```
- How are the blood alcohol variables defined? Interpret the coefficients that you estimate for this concept. 
> The estimated coefficient on *blood alcohol limit 0.08* is negative and significantly different from zero at 5%. Its interpretation is that changing the blood alcohol limit from 0.10% to 0.08% causes traffic fatalities rate to decrease by 0.045924 per 10000 people. 

- Do *per se laws* have a negative effect on the fatality rate? 
> The estimated coefficient on *per se laws* is negative and not significantly different from zero. Its interpretation is that the presence of the per se laws causes traffic fatalities rate to decrease by 0.013310 per 10000 people. 

- Does having a primary seat belt law?
> The estimated coefficient on *primary seatbelt laws* is positive and not significantly different from zero. Its interpretation is that the presence primary seatbelt laws causes traffic fatalities rate to increase by 0.001733 per 10000 people. 

# (15 points) State-Level Fixed Effects 

```{r}
# estimate the fixed effects regression with plm()
mod.fe <- plm(
  log(total_fatalities_rate) ~ year_of_observation
    + factor(blood_alcohol_limit)
    + per_se_laws
    + seatbelt
    + speed_limit_70plus
    + graduated_drivers_license_law
    + pct_population_14_to_24
    + unemployment_rate_log
    + vehicle_miles_per_capita_log,
  data = df,
  index = c("state"),
  model = "within"
)

summary(mod.fe)
# print summary using robust standard errors
coeftest(mod.fe, vcov. = vcovHC, type = "HC1")
```
Re-estimate the **Expanded Model** using fixed effects at the state level. 

- What do you estimate for coefficients on the blood alcohol variables? How do the coefficients on the blood alcohol variables change, if at all? 
> The estimated coefficient on blood alcohol limit 0.08 is negative and not significantly different from zero. Its interpretation is that changing the blood alcohol limit from 0.10% to 0.08% causes traffic fatalities rate to decrease by 0.0022167 per 10000 people. Compared to the expanded model, the coefficient changed from positive to negative and then from significantly different from zero at 5% to not significantly different from zero.

- What do you estimate for coefficients on per se laws? How do the coefficients on per se laws change, if at all? 
> The estimated coefficient on *per se laws* is negative and significantly different from zero at 0%. Its interpretation is that the presence of the per se laws causes traffic fatalities rate to decrease by -0.05703 per 10000 people. Compared to the expanded model, the magnitude of the coefficient because larger and more significantly different from zero.

- What do you estimate for coefficients on primary seat-belt laws? How do the coefficients on primary seatbelt laws change, if at all? 
> The estimated coefficient on *primary seatbelt laws* is negative and significantly different from zero at 0.1%. Its interpretation is that the presence primary seatbelt laws causes traffic fatalities rate to decrease by 0.0392287 per 10000 people. Compared to the expanded model, the coefficient changed from positive to negative and from not significantly different from zero to significantly different from zero at 0.1%. 

Which set of estimates do you think is more reliable? Why do you think this?
> The State-Level Fixed Effects model is more reliable because the sign and magnitude of the estimate changes as we extend the model by entity fixed effects, suggesting that there was some omitted variable bias. Furthermore R^2 increased from 0.6587 to 0.71103 as fixed effects are included in the model equation. 

- What assumptions are needed in each of these models?
> Fixed Effect Model Assumptions:
-**Linearity**: the model is linear in parameters
-**i.i.d.** : The observations are independent across individuals but not necessarily across time. This is guaranteed by random sampling of individuals.
-**Indentifiability**: the regressors, including a constant, are not perfectly collinear, and all regressors (but the constant) have non-zero variance and not too many extreme values.
-**Zero conditional means (strict exogeneity)**


- Are these assumptions reasonable in the current context?
-**Linearity**: This assumption is met since each of the explanatory variable is shown to have a linear relationship with the response variable.
-**i.i.d.** : This assumption is met since the cross-sectional variables measured across state/year on a randomized population.
-**Indentifiability**: In earlier EDA, the selected variables are not perfectly collinear. Using robust standard error on the fitted model, each explanatory variables resulted in a less than 1 standard deviation, which, upon raising it to the power of two, resulted in low non-zero variance. This suggests there are not too many extreme value in the explantory variables. Therefore, this condition is met.
-**Zero conditional means (strict exogeneity)**: Since we're using within estimator to fit a fixed effect model on the state, the effect of unobserved individual heterogeneity is eliminated. Thus this condition is met.


# (10 points) Consider a Random Effects Model 

Instead of estimating a fixed effects model, should you have estimated a random effects model?

- Please state the assumptions of a random effects model, and evaluate whether these assumptions are met in the data. 

> Random Effect Model Assumptions:
- All assumptions under Fixed Effect model
- The unobservbed effect term ai is independent of all explanatory variables in all the time periods in the model

- If the assumptions are, in fact, met in the data, then estimate a random effects model and interpret the coefficients of this model. Comment on how, if at all, the estimates from this model have changed compared to the fixed effects model. 

> The first set of assumption is met under the fixed effect analsysis. To test for the second assumption, we will perform Hausman Test for Fixed vs. Random Effects under the null hypothesis is that the random effects model is acceptable while the alternative hypothesis is there is correlation between residuals and predictors, meaning we should use the fixed effects model. Here, our test resulted in a p-value of 2.649e-7, which is significantly lower than the 95% confidence level, alpha = 0.05. Thus, there are sufficient evidence to reject the Null hypothesis and that the residuals and the predictors are uncorrelated.
> Once fitted, the random effects models yield a lower set of coefficients compared to the fixed effect model. Most notably, the 1981 category variable coefficient for the random effect model is significantly lower than the fixed effect models. Another interesting observation is that the estimated coefficients for seatbelt1, graduated_drivers_license_law and pct_population_14_to_24 become more significant under the t-test, with p-value < 0.01 compared to the fixed effect model. Overall, the changes in the estimated coefficients and their significant could be attribute to the fact that using the random effect model, we're able to captured the variations in fatality rate across states.

```{r random-effect-model estimate}
re.model <- plm(
  log(total_fatalities_rate) ~ year_of_observation
    + factor(blood_alcohol_limit)
    + per_se_laws
    + seatbelt
    + speed_limit_70plus
    + graduated_drivers_license_law
    + pct_population_14_to_24
    + log(unemployment_rate)
    + log(vehicle_miles_per_capita),
  data = df,
  index = c("state"),
  model = "random"
)

phtest(mod.fe, re.model)
summary(re.model)
```

- If the assumptions are **not** met, then do not estimate the data. But, also comment on what the consequences would be if you were to *inappropriately* estimate a random effects model. Would your coefficient estimates be biased or not? Would your standard error estimates be biased or not? Or, would there be some other problem that might arise?



# (10 points) Model Forecasts 

The COVID-19 pandemic dramatically changed patterns of driving. Find data (and include this data in your analysis, here) that includes some measure of vehicle miles driven in the US. Your data should at least cover the period from January 2018 to as current as possible. With this data, produce the following statements: 

- Comparing monthly miles driven in 2018 to the same months during the pandemic: 
  - What month demonstrated the largest decrease in driving? How much, in percentage terms, lower was this driving? 
  - What month demonstrated the largest increase in driving? How much, in percentage terms, higher was this driving? 
  
> We have collected the United States Motor Vehicle Miles Traveled Total (Millions) data from the US Department of Transformation from Jan 1980 to January 2023. This data is available at here [https://www.fhwa.dot.gov/policyinformation/statistics/2021/vm202.cfm]. 

> We have downloaded the data that is compiled by Bloomberg (under ticker: VMTDVCLE Index) and saved it in the file `data/VMTDVCLE.csv`.

```{r monthly-miles-driven-covid}
vehicle_miles_month <- read.csv("./data/VMTDVCLE.csv",
  sep = ",",
  skip = 5
) %>%
  as_tibble() %>%
  mutate(
    vehicle_miles = PX_LAST / 1000, # convert to billions
    Date = as.Date(Date, format = "%m/%d/%Y"),
    year = year(Date),
    month = month(Date)
  ) %>%
  select(year, month, vehicle_miles) %>%
  arrange(year, month)

vehicle_miles_annual <- vehicle_miles_month %>%
  filter(year < 2023) %>% # remove the 2023 data since it's not complete
  group_by(year) %>%
  summarise(total = sum(vehicle_miles))

df_annual <- df %>%
  group_by(year) %>%
  summarise(total = sum(vehicle_miles))

# calculate the avg difference between the two data

combined_mile_annual <- vehicle_miles_annual %>%
  filter(year <= 2004) %>%
  rename(new = total, new_year = year) %>%
  cbind(df_annual) %>%
  mutate(diff = (new - total) / total)

avg_diff <- round(mean(combined_mile_annual$diff) * 100, 1)


# visualize the new vs the old vehicle miles data
ggplot() +
  geom_line(
    data = vehicle_miles_annual,
    aes(x = year, y = total, color = "BBG")
  ) +
  geom_line(
    data = df_annual,
    aes(x = year, y = total, color = "Assignment")
  ) +
  labs(
    title = "US Vehicle Miles Traveled (billions)",
    subtitle = paste0(
      "Data from Bloomberg vs the assignment. Avg difference by year is ~",
      avg_diff, "%"
    ),
    x = "Year",
    y = "Vehicle Miles Traveled (billions)"
  )
```

> We can see the new vehicle milage data is very similar to the old data, and the new data is always higher than the old data. The average difference between the two data is 0.8%.

> This is expected as the new vehicle miles data from Bloomberg include the entire United States, while the old data only include 48 states.

> But we can believe the new data is a good replacement for the old data in our analysis.

```{r monthly-miles-driven-covid-change}
vehicle_miles_month %>%
  filter(year >= 2018 & year < 2023) %>%
  group_by(month) %>%
  summarise(
    year = year,
    chg_vs_2018 = round((vehicle_miles / vehicle_miles[year == 2018] - 1) * 100, 1)
    # vehicle_miles_yoy = round((vehicle_miles / lag(vehicle_miles) - 1) * 100, 1) # yoy
  ) %>%
  filter(year > 2018) %>% # remove the first row
  spread(year, chg_vs_2018) %>%
  knitr::kable()
```

> The largest decrease in driving during COVID bust is in April 2020 (-39.1% compared with April 2018), and the largest increase in driving post-COVID boom is in September 2022 (+5.0% compared with January 2018), which is technically before COVID.

Now, use these changes in driving to make forecasts from your models. 

- Suppose that the number of miles driven per capita, increased by as much as the COVID boom. Using the FE estimates, what would the consequences be on the number of traffic fatalities? Please interpret the estimate.

> Holding everything else constant, if the number of miles driven per capita increased by 5.0%, the number of traffic fatalities would increase by $5% * \beta = 3.8%$. Given the fatalities

- Suppose that the number of miles driven per capita, decreased by as much as the COVID bust. Using the FE estimates, what would the consequences be on the number of traffic fatalities? Please interpret the estimate.

> Holding everything else constant, if the number of miles driven per capita decreased by 39.1%, the number of traffic fatalities would increase by $-39.1% * \beta = -29.5%$. We should be cautious here since the change in miles driven per capita is very large, so the log transformation interpretation might not be accurate.

# (5 points) Evaluate Error 

If there were serial correlation or heteroskedasticity in the idiosyncratic errors of the model, what would be the consequences on the estimators and their standard errors? Is there any serial correlation or heteroskedasticity? 

> If there is serial correlation or heteroskedasticity in the idiosyncratic errors of the model, the standard errors of the estimators will be biased and underestimated.
> #TODO: test the residuals

